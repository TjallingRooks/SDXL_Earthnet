{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doing the exact same as training for the RGB image predictions, but just swapping the images from RGB to NIR.\n",
    "\n",
    "\n",
    "to do in below code:\n",
    "\n",
    "-keep the image_dir\n",
    "\n",
    "-fix the naming: the NIR images contain 'NIR', this needs to be deleted\n",
    "\n",
    "\n",
    "-of note: NIR is a single band image, SDXL expects 3 bands (RGB). So, I had to modify the images to a 3-band file.\n",
    "\n",
    "\n",
    "-difference with RGB model: here we use batch size of 4, is better for the on-the-fly preprocessing we are doing here, since higher batch size causes faster training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\r0902260\\AppData\\Roaming\\Python\\Python311\\site-packages\\xformers\\__init__.py\", line 55, in _is_triton_available\n",
      "    from xformers.triton.softmax import softmax as triton_softmax  # noqa\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\r0902260\\AppData\\Roaming\\Python\\Python311\\site-packages\\xformers\\triton\\softmax.py\", line 11, in <module>\n",
      "    import triton\n",
      "ModuleNotFoundError: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test data points: 1\n",
      "Number of minicubes loaded: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfea7fc5a937490eb3ab5d13e9e3b560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 1\n",
      "Minicube: JAS17_minicube_0_29SND_39.29_-8\n",
      "Starting with image number: 10\n",
      "Input image date: 2017-06-29\n",
      "\n",
      "Generating image 1\n",
      "Current image number: 10\n",
      "Next image number: 11\n",
      "Prompt: Orthophoto taken by a satellite of Santarem in early summer that is drier by 9.80%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3a40ee7e3043b0bb31f035324d1376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image 1 completed\n",
      "\n",
      "Generating image 2\n",
      "Current image number: 11\n",
      "Next image number: 12\n",
      "Prompt: Orthophoto taken by a satellite of Santarem in early summer that is colder by 9.30%, higher pressure by 8.11%, less solar radiation by 17.77%, more humid by 50.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d6520695ce4ddbbb89967621c44c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 175\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNext image number: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 175\u001b[0m generated_image \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m    176\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    177\u001b[0m     image\u001b[38;5;241m=\u001b[39mcurrent_input,\n\u001b[0;32m    178\u001b[0m     strength\u001b[38;5;241m=\u001b[39mstrength,\n\u001b[0;32m    179\u001b[0m     guidance_scale\u001b[38;5;241m=\u001b[39mguidance_scale,\n\u001b[0;32m    180\u001b[0m     num_inference_steps\u001b[38;5;241m=\u001b[39mnum_inference_steps\n\u001b[0;32m    181\u001b[0m )\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    183\u001b[0m generated_images\u001b[38;5;241m.\u001b[39mappend(generated_image)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Prepare the generated image for the next iteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\diffusers\\pipelines\\stable_diffusion_xl\\pipeline_stable_diffusion_xl_img2img.py:982\u001b[0m, in \u001b[0;36mStableDiffusionXLImg2ImgPipeline.__call__\u001b[1;34m(self, prompt, prompt_2, image, strength, num_inference_steps, denoising_start, denoising_end, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, output_type, return_dict, callback, callback_steps, cross_attention_kwargs, guidance_rescale, original_size, crops_coords_top_left, target_size, negative_original_size, negative_crops_coords_top_left, negative_target_size, aesthetic_score, negative_aesthetic_score)\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# predict the noise residual\u001b[39;00m\n\u001b[0;32m    981\u001b[0m added_cond_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m: add_text_embeds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: add_time_ids}\n\u001b[1;32m--> 982\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet(\n\u001b[0;32m    983\u001b[0m     latent_model_input,\n\u001b[0;32m    984\u001b[0m     t,\n\u001b[0;32m    985\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mprompt_embeds,\n\u001b[0;32m    986\u001b[0m     cross_attention_kwargs\u001b[38;5;241m=\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    987\u001b[0m     added_cond_kwargs\u001b[38;5;241m=\u001b[39madded_cond_kwargs,\n\u001b[0;32m    988\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    989\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_classifier_free_guidance:\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\diffusers\\models\\unet_2d_condition.py:1020\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[1;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     upsample_size \u001b[38;5;241m=\u001b[39m down_block_res_samples[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(upsample_block, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_cross_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m upsample_block\u001b[38;5;241m.\u001b[39mhas_cross_attention:\n\u001b[1;32m-> 1020\u001b[0m     sample \u001b[38;5;241m=\u001b[39m upsample_block(\n\u001b[0;32m   1021\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39msample,\n\u001b[0;32m   1022\u001b[0m         temb\u001b[38;5;241m=\u001b[39memb,\n\u001b[0;32m   1023\u001b[0m         res_hidden_states_tuple\u001b[38;5;241m=\u001b[39mres_samples,\n\u001b[0;32m   1024\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1025\u001b[0m         cross_attention_kwargs\u001b[38;5;241m=\u001b[39mcross_attention_kwargs,\n\u001b[0;32m   1026\u001b[0m         upsample_size\u001b[38;5;241m=\u001b[39mupsample_size,\n\u001b[0;32m   1027\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1028\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1029\u001b[0m     )\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1031\u001b[0m     sample \u001b[38;5;241m=\u001b[39m upsample_block(\n\u001b[0;32m   1032\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39msample,\n\u001b[0;32m   1033\u001b[0m         temb\u001b[38;5;241m=\u001b[39memb,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1036\u001b[0m         scale\u001b[38;5;241m=\u001b[39mlora_scale,\n\u001b[0;32m   1037\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\diffusers\\models\\unet_2d_blocks.py:2228\u001b[0m, in \u001b[0;36mCrossAttnUpBlock2D.forward\u001b[1;34m(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states, cross_attention_kwargs, upsample_size, attention_mask, encoder_attention_mask)\u001b[0m\n\u001b[0;32m   2226\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2227\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb, scale\u001b[38;5;241m=\u001b[39mlora_scale)\n\u001b[1;32m-> 2228\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m attn(\n\u001b[0;32m   2229\u001b[0m             hidden_states,\n\u001b[0;32m   2230\u001b[0m             encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   2231\u001b[0m             cross_attention_kwargs\u001b[38;5;241m=\u001b[39mcross_attention_kwargs,\n\u001b[0;32m   2232\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   2233\u001b[0m             encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   2234\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   2235\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   2237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2238\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\diffusers\\models\\transformer_2d.py:315\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, timestep, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m    303\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    304\u001b[0m             block,\n\u001b[0;32m    305\u001b[0m             hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m             use_reentrant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    313\u001b[0m         )\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m block(\n\u001b[0;32m    316\u001b[0m             hidden_states,\n\u001b[0;32m    317\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    318\u001b[0m             encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    319\u001b[0m             encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    320\u001b[0m             timestep\u001b[38;5;241m=\u001b[39mtimestep,\n\u001b[0;32m    321\u001b[0m             cross_attention_kwargs\u001b[38;5;241m=\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    322\u001b[0m             class_labels\u001b[38;5;241m=\u001b[39mclass_labels,\n\u001b[0;32m    323\u001b[0m         )\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\diffusers\\models\\attention.py:218\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     norm_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states, timestep) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ada_layer_norm \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states)\n\u001b[0;32m    216\u001b[0m     )\n\u001b[1;32m--> 218\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn2(\n\u001b[0;32m    219\u001b[0m         norm_hidden_states,\n\u001b[0;32m    220\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    221\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    223\u001b[0m     )\n\u001b[0;32m    224\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m attn_output \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# 4. Feed-forward\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\diffusers\\models\\attention_processor.py:417\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, encoder_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs):\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;66;03m# The `Attention` class can call different attention processors / attention functions\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;66;03m# here we simply pass along all tensors to the selected processor class\u001b[39;00m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# For standard processors that are defined here, `**cross_attention_kwargs` is empty\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m         hidden_states,\n\u001b[0;32m    420\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    421\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    423\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Workdir\\Develop\\anaconda\\Lib\\site-packages\\diffusers\\models\\attention_processor.py:1036\u001b[0m, in \u001b[0;36mAttnProcessor2_0.__call__\u001b[1;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, scale)\u001b[0m\n\u001b[0;32m   1032\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attn\u001b[38;5;241m.\u001b[39mheads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# the output of sdp = (batch, num_heads, seq_len, head_dim)\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m# TODO: add support for attn.scale when we move to Torch 2.1\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[0;32m   1037\u001b[0m     query, key, value, attn_mask\u001b[38;5;241m=\u001b[39mattention_mask, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m )\n\u001b[0;32m   1040\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attn\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m*\u001b[39m head_dim)\n\u001b[0;32m   1041\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage\n",
    "csv_path = r\"C:\\TjallingData\\greenearthnet\\metadata_total_imputed_with_seasons_usable_5050.csv\"\n",
    "image_dir = r\"C:\\TjallingData\\greenearthnet\\train\\NIR_combined\"\n",
    "max_minicubes = None  \n",
    "\n",
    "\n",
    "\n",
    "class SatelliteImageDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_dir, max_minicubes=None):\n",
    "        self.data = []\n",
    "        self.image_dir = image_dir\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((1024, 1024)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "        \n",
    "        self.metadata = self.load_metadata(csv_path)\n",
    "        minicube_data = {}\n",
    "        minicube_count = 0\n",
    "\n",
    "        with open(csv_path, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                if row['usable'].lower() == 'yes':\n",
    "                    minicube = row['minicube']\n",
    "                    if minicube not in minicube_data:\n",
    "                        minicube_data[minicube] = []\n",
    "                        minicube_count += 1\n",
    "                        if max_minicubes and minicube_count > max_minicubes:\n",
    "                            break\n",
    "                    \n",
    "                    # Construct the filename with _NIR before the date\n",
    "                    file_name = f\"{minicube}_NIR_{row['frame_date']}.png\"\n",
    "                    minicube_data[minicube].append((file_name, row['frame_date'], int(row['number'])))\n",
    "\n",
    "        for minicube, images in minicube_data.items():\n",
    "            sorted_images = sorted(images, key=lambda x: x[2])  # Sort by image number\n",
    "            for i in range(len(sorted_images) - 1):\n",
    "                self.data.append((minicube, *sorted_images[i], *sorted_images[i+1][1:]))\n",
    "        \n",
    "        print(f\"Total usable data points: {len(self.data)}\")\n",
    "        print(f\"Number of minicubes loaded: {minicube_count}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        minicube, input_file, input_date, input_number, target_date, target_number = self.data[idx]\n",
    "        \n",
    "        # Use the correct file naming convention\n",
    "        input_image = self.load_image(input_file)\n",
    "        target_image = self.load_image(f\"{minicube}_NIR_{target_date}.png\")\n",
    "        \n",
    "        # Convert single-channel images to 3-channel\n",
    "        input_image = input_image.repeat(3, 1, 1)\n",
    "        target_image = target_image.repeat(3, 1, 1)\n",
    "        \n",
    "        input_image = input_image.requires_grad_(True)\n",
    "        \n",
    "        prompt, input_num, target_num = self.generate_prompt(minicube, str(input_number), str(target_number))\n",
    "        \n",
    "        input_climate = self.get_metadata(minicube, str(input_number))\n",
    "        target_climate = self.get_metadata(minicube, str(target_number))\n",
    "        \n",
    "        print(f\"Input image: {input_file}, Target image: {minicube}_NIR_{target_date}.png\")\n",
    "        print(f\"Input image number: {input_number}, Target image number: {target_number}\")\n",
    "        print(f\"Input climate data: {input_climate}\")\n",
    "        print(f\"Target climate data: {target_climate}\")\n",
    "        print(f\"Full prompt: {prompt}\")\n",
    "        \n",
    "        return input_image, target_image, input_number, target_number, prompt\n",
    "\n",
    "    def load_image(self, filename):\n",
    "        path = os.path.join(self.image_dir, filename)\n",
    "        \n",
    "        if os.path.exists(path):\n",
    "            # Load as grayscale image\n",
    "            image = Image.open(path).convert(\"L\")\n",
    "            image = self.transform(image)\n",
    "            return image\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "\n",
    "    def load_metadata(self, metadata_csv_path):\n",
    "        metadata = {}\n",
    "        with open(metadata_csv_path, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                minicube = row['minicube'] \n",
    "                number = row['number']\n",
    "                \n",
    "                if minicube not in metadata:\n",
    "                    metadata[minicube] = {}\n",
    "                metadata[minicube][number] = {\n",
    "                    'Temperature Avg': float(row['Temperature Avg']),\n",
    "                    'Sea-level Pressure': float(row['Sea-level Pressure']),\n",
    "                    'Shortwave Downwelling Radiation': float(row['Shortwave Downwelling Radiation']),\n",
    "                    'Relative Humidity': float(row['Relative Humidity']),\n",
    "                    'region': row['region'],\n",
    "                    'season': row['season'],\n",
    "                    'number': row['number']\n",
    "                }\n",
    "        return metadata\n",
    "\n",
    "    def get_metadata(self, minicube, number):\n",
    "        if minicube in self.metadata and number in self.metadata[minicube]:\n",
    "            return self.metadata[minicube][number]\n",
    "        else:\n",
    "            print(f\"Warning: Metadata not found for minicube {minicube}, number {number}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    to fix: if the images are similar (differences all under 5%), make a specific prompt for it\n",
    "    also, for NIR, should specify different prompt\n",
    "    '''\n",
    "\n",
    "    def generate_prompt(self, minicube, input_number, target_number):\n",
    "        input_data = self.get_metadata(minicube, input_number)\n",
    "        target_data = self.get_metadata(minicube, target_number)\n",
    "        \n",
    "        if input_data is None or target_data is None:\n",
    "            return \"Orthophoto taken by a satellite of a nature region\", None, None\n",
    "        \n",
    "        differences = []\n",
    "        for key in ['Temperature Avg', 'Sea-level Pressure', 'Shortwave Downwelling Radiation', 'Relative Humidity']:\n",
    "            if key in input_data and key in target_data:\n",
    "                diff = (target_data[key] - input_data[key]) / input_data[key] * 100  # Convert to percentage change\n",
    "                if abs(diff) > 5:  # Only include significant changes (5%)\n",
    "                    if key == 'Temperature Avg':\n",
    "                        differences.append(f\"{'warmer' if diff > 0 else 'colder'} by {abs(diff):.2f}%\")\n",
    "                    elif key == 'Sea-level Pressure':\n",
    "                        differences.append(f\"{'higher pressure' if diff > 0 else 'lower pressure'} by {abs(diff):.2f}%\")\n",
    "                    elif key == 'Shortwave Downwelling Radiation':\n",
    "                        differences.append(f\"{'more solar radiation' if diff > 0 else 'less solar radiation'} by {abs(diff):.2f}%\")\n",
    "                    elif key == 'Relative Humidity':\n",
    "                        differences.append(f\"{'more humid' if diff > 0 else 'drier'} by {abs(diff):.2f}%\")\n",
    "        \n",
    "        region = input_data.get('region', 'unknown region')\n",
    "        season = input_data.get('season', 'unknown season')\n",
    "        prompt = f\"Orthophoto taken by a satellite of {region} in {season} that is \" + \", \".join(differences)\n",
    "        return prompt, input_data['number'], target_data['number']\n",
    "\n",
    "# Create dataset with a limit on the number of minicubes\n",
    "max_minicubes = None  \n",
    "dataset = SatelliteImageDataset(csv_path, image_dir, max_minicubes=max_minicubes)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Load the model\n",
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "pipeline = StableDiffusionXLImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipeline = pipeline.to(\"cuda\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "optimizer = torch.optim.AdamW(pipeline.unet.parameters(), lr=1e-5)\n",
    "\n",
    "# hyperparameters\n",
    "strength = 0.3  \n",
    "guidance_scale = 3.0  \n",
    "num_inference_steps = 50  \n",
    "\n",
    "# Checkpointing parameters\n",
    "checkpoint_interval = 10  # Save checkpoint every 10 batches\n",
    "checkpoint_path = \"model_checkpoint_NIR\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (input_image, target_image, input_number, target_number, prompt) in enumerate(dataloader):\n",
    "        input_image = input_image.to(\"cuda\")\n",
    "        target_image = target_image.to(\"cuda\")\n",
    "        \n",
    "        # Generate image\n",
    "        generated_images = pipeline(\n",
    "            prompt=prompt[0],  # Use the generated prompt\n",
    "            image=input_image,\n",
    "            strength=strength,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps\n",
    "        ).images\n",
    "\n",
    "        # Convert output image to tensor and ensure it requires gradients\n",
    "        output_tensor = transforms.ToTensor()(generated_images[0]).to(\"cuda\").requires_grad_(True)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = torch.nn.functional.mse_loss(output_tensor, target_image)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Input: {input_number.item()}, Target: {target_number.item()}, Loss: {loss.item()}\")\n",
    "\n",
    "        # Checkpointing\n",
    "        if (batch_idx + 1) % checkpoint_interval == 0:\n",
    "            pipeline.save_pretrained(checkpoint_path)\n",
    "            torch.save(optimizer.state_dict(), os.path.join(checkpoint_path, \"optimizer_state.pth\"))\n",
    "            print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = SatelliteImageDataset(csv_path, image_dir, max_minicubes=max_minicubes)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=4)\n",
    "\n",
    "# Save the final trained model and optimizer state\n",
    "pipeline.save_pretrained(\"trained_satellite_model_with_prompt_normalized_5050_NIR\")\n",
    "torch.save(optimizer.state_dict(), \"optimizer_state_NIR.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load it back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Paths\n",
    "model_path = \"trained_satellite_model_with_prompt\"\n",
    "checkpoint_path = \"model_checkpoint_NIR\"\n",
    "input_image_path = r\"C:\\Users\\r0902260\\TjallingData\\train\\satellite_images_combined_borderless\\29SND_2017-06-10_2017-11-06_2105_2233_2873_3001_32_112_44_124_2017-06-14.png\"\n",
    "\n",
    "# Load the model\n",
    "pipeline = StableDiffusionXLImg2ImgPipeline.from_pretrained(checkpoint_path, torch_dtype=torch.float16)\n",
    "pipeline = pipeline.to(\"cuda\")\n",
    "\n",
    "# Load and preprocess the input image\n",
    "input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Define the preprocessing transforms\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((1024, 1024)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Apply the preprocessing\n",
    "input_tensor = preprocess(input_image).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "# Perform inference\n",
    "prompt = \"Orthophoto taken of SantarAm by a satellite in late spring that is drier by 9.23%\"\n",
    "strength = 0.3\n",
    "guidance_scale = 3.0\n",
    "num_inference_steps = 50\n",
    "\n",
    "generated_images = pipeline(\n",
    "    prompt=prompt,\n",
    "    image=input_tensor,\n",
    "    strength=strength,\n",
    "    guidance_scale=guidance_scale,\n",
    "    num_inference_steps=num_inference_steps\n",
    ").images\n",
    "\n",
    "# Save or display the generated image\n",
    "output_image = generated_images[0]\n",
    "output_image.save(\"output_image_teacherforcing_checkpoint.png\")\n",
    "output_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
