{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first a quality check to see if there are the exact same amounts of NIR bands as there are RGB bands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def count_files_in_subfolder(path):\n",
    "    try:\n",
    "        return set(f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)))\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "\n",
    "def load_csv_data(csv_file):\n",
    "    non_imputed_files = set()\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if row['imputed'].lower() == 'no':\n",
    "                non_imputed_files.add(row['file_name'])\n",
    "    return non_imputed_files\n",
    "\n",
    "def check_file_consistency(base_dir, folders):\n",
    "    for folder in folders:\n",
    "        full_path = os.path.join(base_dir, folder)\n",
    "        nir_path = os.path.join(full_path, 'NIR')\n",
    "        rgb_path = os.path.join(full_path, 'RGB')\n",
    "        csv_file = os.path.join(full_path, f\"{folder}_detailed.csv\")\n",
    "        \n",
    "        nir_files = count_files_in_subfolder(nir_path)\n",
    "        rgb_files = count_files_in_subfolder(rgb_path)\n",
    "        non_imputed_files = load_csv_data(csv_file)\n",
    "        \n",
    "        print(f\"Folder: {folder}\")\n",
    "        print(f\"  NIR files: {len(nir_files)}\")\n",
    "        print(f\"  RGB files: {len(rgb_files)}\")\n",
    "        print(f\"  Non-imputed files in CSV: {len(non_imputed_files)}\")\n",
    "        \n",
    "        nir_missing = non_imputed_files - nir_files\n",
    "        rgb_missing = non_imputed_files - rgb_files\n",
    "        extra_nir = nir_files - non_imputed_files\n",
    "        extra_rgb = rgb_files - non_imputed_files\n",
    "        \n",
    "        print(\"  Missing non-imputed files in NIR:\", len(nir_missing))\n",
    "        if nir_missing:\n",
    "            print(\"    \", \", \".join(list(nir_missing)[:5]), \"...\" if len(nir_missing) > 5 else \"\")\n",
    "        \n",
    "        print(\"  Missing non-imputed files in RGB:\", len(rgb_missing))\n",
    "        if rgb_missing:\n",
    "            print(\"    \", \", \".join(list(rgb_missing)[:5]), \"...\" if len(rgb_missing) > 5 else \"\")\n",
    "        \n",
    "        print(\"  Extra files in NIR:\", len(extra_nir))\n",
    "        if extra_nir:\n",
    "            print(\"    \", \", \".join(list(extra_nir)[:5]), \"...\" if len(extra_nir) > 5 else \"\")\n",
    "        \n",
    "        print(\"  Extra files in RGB:\", len(extra_rgb))\n",
    "        if extra_rgb:\n",
    "            print(\"    \", \", \".join(list(extra_rgb)[:5]), \"...\" if len(extra_rgb) > 5 else \"\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Base directory\n",
    "output_base_dir = r'C:\\TjallingData\\greenearthnet_additional'\n",
    "\n",
    "# Folders to check\n",
    "folders_to_check = [\n",
    "    'iid_chopped',\n",
    "    'ood-s_chopped',\n",
    "    'ood-st_chopped',\n",
    "    'ood-t_chopped',\n",
    "    'val_chopped',\n",
    "    'train'\n",
    "]\n",
    "\n",
    "check_file_consistency(output_base_dir, folders_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality check showed more NIR images than RGB images being extracted, due to them being less sensitive to the cloud cover. The excess NIR images will be deleted in the cell below and imputed afterwards, to ensure consistency towards the RGB images. Below is the same code as above, but with an extra delete function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def count_files_in_subfolder(path):\n",
    "    try:\n",
    "        return set(f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)))\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "\n",
    "def load_csv_data(csv_file):\n",
    "    non_imputed_files = set()\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if row['imputed'].lower() == 'no':\n",
    "                non_imputed_files.add(row['file_name'])\n",
    "    return non_imputed_files\n",
    "\n",
    "def check_file_consistency(base_dir, folders):\n",
    "    for folder in folders:\n",
    "        full_path = os.path.join(base_dir, folder)\n",
    "        nir_path = os.path.join(full_path, 'NIR')\n",
    "        rgb_path = os.path.join(full_path, 'RGB')\n",
    "        csv_file = os.path.join(full_path, f\"{folder}_detailed.csv\")\n",
    "        \n",
    "        nir_files = count_files_in_subfolder(nir_path)\n",
    "        rgb_files = count_files_in_subfolder(rgb_path)\n",
    "        non_imputed_files = load_csv_data(csv_file)\n",
    "        \n",
    "        print(f\"Folder: {folder}\")\n",
    "        print(f\"  NIR files: {len(nir_files)}\")\n",
    "        print(f\"  RGB files: {len(rgb_files)}\")\n",
    "        print(f\"  Non-imputed files in CSV: {len(non_imputed_files)}\")\n",
    "        \n",
    "        nir_missing = non_imputed_files - nir_files\n",
    "        rgb_missing = non_imputed_files - rgb_files\n",
    "        extra_nir = nir_files - non_imputed_files\n",
    "        extra_rgb = rgb_files - non_imputed_files\n",
    "        \n",
    "        print(\"  Missing non-imputed files in NIR:\", len(nir_missing))\n",
    "        if nir_missing:\n",
    "            print(\"    \", \", \".join(list(nir_missing)[:5]), \"...\" if len(nir_missing) > 5 else \"\")\n",
    "        \n",
    "        print(\"  Missing non-imputed files in RGB:\", len(rgb_missing))\n",
    "        if rgb_missing:\n",
    "            print(\"    \", \", \".join(list(rgb_missing)[:5]), \"...\" if len(rgb_missing) > 5 else \"\")\n",
    "        \n",
    "        print(\"  Extra files in NIR:\", len(extra_nir))\n",
    "        if extra_nir:\n",
    "            print(\"    \", \", \".join(list(extra_nir)[:5]), \"...\" if len(extra_nir) > 5 else \"\")\n",
    "        \n",
    "        print(\"  Extra files in RGB:\", len(extra_rgb))\n",
    "        if extra_rgb:\n",
    "            print(\"    \", \", \".join(list(extra_rgb)[:5]), \"...\" if len(extra_rgb) > 5 else \"\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "def delete_extra_nir_files(base_dir, folders):\n",
    "    for folder in folders:\n",
    "        full_path = os.path.join(base_dir, folder)\n",
    "        nir_path = os.path.join(full_path, 'NIR')\n",
    "        csv_file = os.path.join(full_path, f\"{folder}_detailed.csv\")\n",
    "        \n",
    "        nir_files = count_files_in_subfolder(nir_path)\n",
    "        non_imputed_files = load_csv_data(csv_file)\n",
    "        \n",
    "        extra_nir = nir_files - non_imputed_files\n",
    "        \n",
    "        print(f\"Folder: {folder}\")\n",
    "        print(f\"  Extra files to delete in NIR: {len(extra_nir)}\")\n",
    "        \n",
    "        for file in extra_nir:\n",
    "            file_path = os.path.join(nir_path, file)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"    Deleted: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error deleting {file}: {str(e)}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Base directory\n",
    "output_base_dir = r'C:\\TjallingData\\greenearthnet_additional'\n",
    "\n",
    "# Folders to check\n",
    "folders_to_check = [\n",
    "    'iid_chopped',\n",
    "    'ood-s_chopped',\n",
    "    'ood-st_chopped',\n",
    "    'ood-t_chopped',\n",
    "    'val_chopped',\n",
    "    'train'\n",
    "]\n",
    "\n",
    "# First cell: Check file consistency\n",
    "print(\"Checking file consistency:\")\n",
    "check_file_consistency(output_base_dir, folders_to_check)\n",
    "\n",
    "# Second cell: Delete extra NIR files\n",
    "print(\"\\nDeleting extra NIR files:\")\n",
    "delete_extra_nir_files(output_base_dir, folders_to_check)\n",
    "\n",
    "# Optional: Check file consistency again after deletion\n",
    "print(\"\\nChecking file consistency after deletion:\")\n",
    "check_file_consistency(output_base_dir, folders_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the NIR image imputation takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def load_csv_data(csv_file):\n",
    "    data = {}\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            minicube_name = row['file_name'].split('_')[1:5]\n",
    "            minicube_name = '_'.join(minicube_name)\n",
    "            if minicube_name not in data:\n",
    "                data[minicube_name] = []\n",
    "            data[minicube_name].append(row)\n",
    "    return data\n",
    "\n",
    "def impute_image(before_img, after_img, total_gap, current_gap):\n",
    "    before_arr = np.array(before_img)\n",
    "    after_arr = np.array(after_img)\n",
    "    \n",
    "    if total_gap == 0:\n",
    "        return before_arr\n",
    "    \n",
    "    before_weight = 1 - (current_gap / total_gap)\n",
    "    after_weight = current_gap / total_gap\n",
    "    imputed_arr = (before_weight * before_arr + after_weight * after_arr).astype(np.uint8)\n",
    "    return imputed_arr\n",
    "\n",
    "\n",
    "def process_folder(input_folder, output_folder, csv_file):\n",
    "    csv_data = load_csv_data(csv_file)\n",
    "    \n",
    "    for minicube_name, minicube_data in csv_data.items():\n",
    "        print(f\"Processing minicube: {minicube_name}\")\n",
    "        \n",
    "        # Sort minicube data by date\n",
    "        minicube_data.sort(key=lambda x: x['date'])\n",
    "        \n",
    "        existing_dates = [datetime.strptime(row['date'], '%Y-%m-%d').date() for row in minicube_data]\n",
    "        first_date = min(existing_dates)\n",
    "        last_date = max(existing_dates)\n",
    "        \n",
    "        for i, row in enumerate(minicube_data):\n",
    "            if row['imputed'] == 'yes':\n",
    "                print(f\"Imputing image: {row['file_name']}\")\n",
    "                current_date = datetime.strptime(row['date'], '%Y-%m-%d').date()\n",
    "                \n",
    "                # Find the nearest non-imputed images before and after\n",
    "                before = max((d for d in existing_dates if d <= current_date and minicube_data[existing_dates.index(d)]['imputed'] == 'no'), default=first_date)\n",
    "                after = min((d for d in existing_dates if d >= current_date and minicube_data[existing_dates.index(d)]['imputed'] == 'no'), default=last_date)\n",
    "                \n",
    "                before_idx = existing_dates.index(before)\n",
    "                after_idx = existing_dates.index(after)\n",
    "                \n",
    "                before_row = minicube_data[before_idx]\n",
    "                after_row = minicube_data[after_idx]\n",
    "                \n",
    "                # Try to open images, use fallback if file not found\n",
    "                before_img = open_image_with_fallback(input_folder, before_row['file_name'])\n",
    "                after_img = open_image_with_fallback(input_folder, after_row['file_name'])\n",
    "                \n",
    "                if before_img is None or after_img is None:\n",
    "                    print(f\"Warning: Could not impute image {row['file_name']} due to missing reference images\")\n",
    "                    continue\n",
    "                \n",
    "                total_gap = (after - before).days\n",
    "                current_gap = (current_date - before).days\n",
    "                \n",
    "                if total_gap == 0:\n",
    "                    imputed_arr = np.array(before_img)\n",
    "                else:\n",
    "                    before_weight = 1 - (current_gap / total_gap)\n",
    "                    after_weight = current_gap / total_gap\n",
    "                    imputed_arr = (before_weight * np.array(before_img) + after_weight * np.array(after_img)).astype(np.uint8)\n",
    "                \n",
    "                imputed_img = Image.fromarray(imputed_arr)\n",
    "                \n",
    "                # Save the imputed image\n",
    "                output_filename = f\"NIR_imputed_{row['file_name']}\"\n",
    "                output_path = os.path.join(output_folder, output_filename)\n",
    "                imputed_img.save(output_path)\n",
    "                print(f\"Saved imputed image: {output_path}\")\n",
    "        \n",
    "        # Verify the total number of images\n",
    "        total_images = len([f for f in os.listdir(input_folder) if f.endswith('.png')])\n",
    "        total_imputed = len([f for f in os.listdir(output_folder) if f.startswith('NIR_imputed_')])\n",
    "        print(f\"Total images: {total_images}, Imputed images: {total_imputed}\")\n",
    "        if total_images + total_imputed != 30:\n",
    "            print(f\"Warning: Incorrect total number of images for {minicube_name}. Expected 30, got {total_images + total_imputed}\")\n",
    "\n",
    "def open_image_with_fallback(folder, filename):\n",
    "    try:\n",
    "        return Image.open(os.path.join(folder, filename))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File not found: {filename}\")\n",
    "        # You can implement a fallback method here if needed\n",
    "        # For example, return a blank image or the most recent available image\n",
    "        return None\n",
    "\n",
    "# The rest of the code remains the same\n",
    "\n",
    "# List of folders to process\n",
    "folders = [\n",
    "    r'C:\\TjallingData\\greenearthnet_additional\\iid_chopped',\n",
    "    r'C:\\TjallingData\\greenearthnet_additional\\ood-s_chopped',\n",
    "    r'C:\\TjallingData\\greenearthnet_additional\\ood-st_chopped',\n",
    "    r'C:\\TjallingData\\greenearthnet_additional\\ood-t_chopped',\n",
    "    r'C:\\TjallingData\\greenearthnet_additional\\val_chopped',\n",
    "    r'C:\\TjallingData\\greenearthnet_additional\\train',\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "for folder in folders:\n",
    "    print(f\"Processing folder: {folder}\")\n",
    "    input_folder = os.path.join(folder, 'NIR')\n",
    "    output_folder = os.path.join(folder, 'NIR_imputed')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    csv_file = os.path.join(folder, f\"{os.path.basename(folder)}_detailed.csv\")\n",
    "    \n",
    "    process_folder(input_folder, output_folder, csv_file)\n",
    "\n",
    "print(\"Imputation process completed for all folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doing the exact same kind of imputation for RGB images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def load_csv_data(csv_file):\n",
    "    data = {}\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            minicube_name = row['file_name'].split('_')[1:5]\n",
    "            minicube_name = '_'.join(minicube_name)\n",
    "            if minicube_name not in data:\n",
    "                data[minicube_name] = []\n",
    "            data[minicube_name].append(row)\n",
    "    return data\n",
    "\n",
    "def impute_image(before_img, after_img, total_gap, current_gap):\n",
    "    before_arr = np.array(before_img)\n",
    "    after_arr = np.array(after_img)\n",
    "    \n",
    "    if total_gap == 0:\n",
    "        return before_arr\n",
    "    \n",
    "    before_weight = 1 - (current_gap / total_gap)\n",
    "    after_weight = current_gap / total_gap\n",
    "    imputed_arr = (before_weight * before_arr + after_weight * after_arr).astype(np.uint8)\n",
    "    return imputed_arr\n",
    "\n",
    "def process_folder(input_folder, output_folder, csv_file):\n",
    "    csv_data = load_csv_data(csv_file)\n",
    "    \n",
    "    for minicube_name, minicube_data in csv_data.items():\n",
    "        print(f\"Processing minicube: {minicube_name}\")\n",
    "        \n",
    "        # Sort minicube data by date\n",
    "        minicube_data.sort(key=lambda x: x['date'])\n",
    "        \n",
    "        existing_dates = [datetime.strptime(row['date'], '%Y-%m-%d').date() for row in minicube_data]\n",
    "        first_date = min(existing_dates)\n",
    "        last_date = max(existing_dates)\n",
    "        \n",
    "        for i, row in enumerate(minicube_data):\n",
    "            if row['imputed'] == 'yes':\n",
    "                output_filename = f\"RGB_imputed_{row['file_name']}\"\n",
    "                output_path = os.path.join(output_folder, output_filename)\n",
    "                \n",
    "                # Check if the imputed image already exists\n",
    "                if os.path.exists(output_path):\n",
    "                    print(f\"Skipping existing imputed image: {output_filename}\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Imputing image: {row['file_name']}\")\n",
    "                current_date = datetime.strptime(row['date'], '%Y-%m-%d').date()\n",
    "                \n",
    "                # Find the nearest non-imputed images before and after\n",
    "                before = max((d for d in existing_dates if d <= current_date and minicube_data[existing_dates.index(d)]['imputed'] == 'no'), default=first_date)\n",
    "                after = min((d for d in existing_dates if d >= current_date and minicube_data[existing_dates.index(d)]['imputed'] == 'no'), default=last_date)\n",
    "                \n",
    "                before_idx = existing_dates.index(before)\n",
    "                after_idx = existing_dates.index(after)\n",
    "                \n",
    "                before_row = minicube_data[before_idx]\n",
    "                after_row = minicube_data[after_idx]\n",
    "                \n",
    "                # Try to open images, use fallback if file not found\n",
    "                before_img = open_image_with_fallback(input_folder, before_row['file_name'])\n",
    "                after_img = open_image_with_fallback(input_folder, after_row['file_name'])\n",
    "                \n",
    "                if before_img is None or after_img is None:\n",
    "                    print(f\"Warning: Could not impute image {row['file_name']} due to missing reference images\")\n",
    "                    continue\n",
    "                \n",
    "                total_gap = (after - before).days\n",
    "                current_gap = (current_date - before).days\n",
    "                \n",
    "                if total_gap == 0:\n",
    "                    imputed_arr = np.array(before_img)\n",
    "                else:\n",
    "                    before_weight = 1 - (current_gap / total_gap)\n",
    "                    after_weight = current_gap / total_gap\n",
    "                    imputed_arr = (before_weight * np.array(before_img) + after_weight * np.array(after_img)).astype(np.uint8)\n",
    "                \n",
    "                imputed_img = Image.fromarray(imputed_arr)\n",
    "                \n",
    "                # Save the imputed image\n",
    "                imputed_img.save(output_path)\n",
    "                print(f\"Saved imputed image: {output_path}\")\n",
    "        \n",
    "        # Verify the total number of images\n",
    "        total_images = len([f for f in os.listdir(input_folder) if f.endswith('.png')])\n",
    "        total_imputed = len([f for f in os.listdir(output_folder) if f.startswith('RGB_imputed_')])\n",
    "        print(f\"Total images: {total_images}, Imputed images: {total_imputed}\")\n",
    "        if total_images + total_imputed != 30:\n",
    "            print(f\"Warning: Incorrect total number of images for {minicube_name}. Expected 30, got {total_images + total_imputed}\")\n",
    "\n",
    "def open_image_with_fallback(folder, filename):\n",
    "    try:\n",
    "        return Image.open(os.path.join(folder, filename))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File not found: {filename}\")\n",
    "        # You can implement a fallback method here if needed\n",
    "        # For example, return a blank image or the most recent available image\n",
    "        return None\n",
    "\n",
    "def count_existing_imputed(output_folder):\n",
    "    return len([f for f in os.listdir(output_folder) if f.startswith('RGB_imputed_')])\n",
    "\n",
    "# List of folders to process\n",
    "folders = [\n",
    "    r'C:\\TjallingData\\greenearthnet_additional\\iid_chopped',\n",
    "    r'C:\\TjallingData\\greenearthnet_additional\\ood-s_chopped',\n",
    "    r'C:\\TjallingData\\greenearthnet_additional\\ood-st_chopped',\n",
    "    r'C:\\TjallingData\\greenearthnet_additional\\ood-t_chopped',\n",
    "    r'C:\\TjallingData\\greenearthnet_additional\\val_chopped',\n",
    "    r'C:\\TjallingData\\greenearthnet_additional\\train'\n",
    "\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    print(f\"Processing folder: {folder}\")\n",
    "    input_folder = os.path.join(folder, 'RGB')\n",
    "    output_folder = os.path.join(folder, 'RGB_imputed')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    csv_file = os.path.join(folder, f\"{os.path.basename(folder)}_detailed.csv\")\n",
    "    \n",
    "    existing_imputed = count_existing_imputed(output_folder)\n",
    "    print(f\"Found {existing_imputed} existing imputed images\")\n",
    "    \n",
    "    process_folder(input_folder, output_folder, csv_file)\n",
    "    \n",
    "    final_imputed = count_existing_imputed(output_folder)\n",
    "    print(f\"Folder completed. Total imputed images: {final_imputed}\")\n",
    "\n",
    "print(\"Imputation process completed for all folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "post-imputing quality check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def count_files_in_subfolder(path):\n",
    "    try:\n",
    "        return set(f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)))\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "\n",
    "def load_csv_data(csv_file):\n",
    "    non_imputed_files = set()\n",
    "    imputed_files = set()\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if row['imputed'].lower() == 'no':\n",
    "                non_imputed_files.add(row['file_name'])\n",
    "            else:\n",
    "                imputed_files.add(row['file_name'])\n",
    "    return non_imputed_files, imputed_files\n",
    "\n",
    "def check_folder_consistency(folder_path, files_in_csv, imputed):\n",
    "    folder_files = count_files_in_subfolder(folder_path)\n",
    "    prefix = \"imputed_\" if imputed else \"\"\n",
    "    expected_files = {f\"{prefix}{f}\" for f in files_in_csv}\n",
    "    \n",
    "    missing_files = expected_files - folder_files\n",
    "    extra_files = folder_files - expected_files\n",
    "    \n",
    "    return folder_files, missing_files, extra_files\n",
    "\n",
    "def print_results(folder_name, folder_files, missing_files, extra_files):\n",
    "    print(f\"  {folder_name} files: {len(folder_files)}\")\n",
    "    print(f\"  Missing files in {folder_name}:\", len(missing_files))\n",
    "    if missing_files:\n",
    "        print(\"    \", \", \".join(list(missing_files)[:5]), \"...\" if len(missing_files) > 5 else \"\")\n",
    "    print(f\"  Extra files in {folder_name}:\", len(extra_files))\n",
    "    if extra_files:\n",
    "        print(\"    \", \", \".join(list(extra_files)[:5]), \"...\" if len(extra_files) > 5 else \"\")\n",
    "\n",
    "def check_file_consistency(base_dir, folders):\n",
    "    for folder in folders:\n",
    "        full_path = os.path.join(base_dir, folder)\n",
    "        csv_file = os.path.join(full_path, f\"{folder}_detailed.csv\")\n",
    "        \n",
    "        non_imputed_files, imputed_files = load_csv_data(csv_file)\n",
    "        \n",
    "        print(f\"Folder: {folder}\")\n",
    "        print(f\"  Non-imputed files in CSV: {len(non_imputed_files)}\")\n",
    "        print(f\"  Imputed files in CSV: {len(imputed_files)}\")\n",
    "        \n",
    "        for subfolder in ['NIR', 'NIR_imputed', 'RGB', 'RGB_imputed']:\n",
    "            subfolder_path = os.path.join(full_path, subfolder)\n",
    "            is_imputed = 'imputed' in subfolder.lower()\n",
    "            files_in_csv = imputed_files if is_imputed else non_imputed_files\n",
    "            \n",
    "            folder_files, missing_files, extra_files = check_folder_consistency(subfolder_path, files_in_csv, is_imputed)\n",
    "            print_results(subfolder, folder_files, missing_files, extra_files)\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Base directory\n",
    "output_base_dir = r'C:\\TjallingData\\greenearthnet_additional'\n",
    "\n",
    "# Folders to check\n",
    "folders_to_check = [\n",
    "    'iid_chopped',\n",
    "    'ood-s_chopped',\n",
    "    'ood-st_chopped',\n",
    "    'ood-t_chopped',\n",
    "    'val_chopped'\n",
    "]\n",
    "\n",
    "check_file_consistency(output_base_dir, folders_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the post-imputation quality check, it is obvious many imputed NIR images are missing. in the text below, I explicitly try to impute these to see where it goes wrong.\n",
    "\n",
    "First I create a .csv file containing all missing images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def count_files_in_subfolder(path):\n",
    "    try:\n",
    "        return set(f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)))\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "\n",
    "def load_csv_data(csv_file):\n",
    "    non_imputed_files = set()\n",
    "    imputed_files = set()\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if row['imputed'].lower() == 'no':\n",
    "                non_imputed_files.add(row['file_name'])\n",
    "            else:\n",
    "                imputed_files.add(row['file_name'])\n",
    "    return non_imputed_files, imputed_files\n",
    "\n",
    "def save_missing_imputed(missing_files, output_file):\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['missing_file'])\n",
    "        for file in sorted(missing_files):\n",
    "            writer.writerow([file])\n",
    "\n",
    "def check_file_consistency(base_dir, folders):\n",
    "    for folder in folders:\n",
    "        full_path = os.path.join(base_dir, folder)\n",
    "        nir_path = os.path.join(full_path, 'NIR')\n",
    "        nir_imputed_path = os.path.join(full_path, 'NIR_imputed')\n",
    "        csv_file = os.path.join(full_path, f\"{folder}_detailed.csv\")\n",
    "        \n",
    "        nir_files = count_files_in_subfolder(nir_path)\n",
    "        nir_imputed_files = count_files_in_subfolder(nir_imputed_path)\n",
    "        non_imputed_files, imputed_files = load_csv_data(csv_file)\n",
    "        \n",
    "        print(f\"Folder: {folder}\")\n",
    "        print(f\"  NIR files: {len(nir_files)}\")\n",
    "        print(f\"  NIR_imputed files: {len(nir_imputed_files)}\")\n",
    "        print(f\"  Non-imputed files in CSV: {len(non_imputed_files)}\")\n",
    "        print(f\"  Imputed files in CSV: {len(imputed_files)}\")\n",
    "        \n",
    "        nir_missing = non_imputed_files - nir_files\n",
    "        nir_imputed_missing = {f\"NIR_imputed_{f}\" for f in imputed_files} - nir_imputed_files\n",
    "        extra_nir = nir_files - non_imputed_files\n",
    "        extra_nir_imputed = nir_imputed_files - {f\"NIR_imputed_{f}\" for f in imputed_files}\n",
    "        \n",
    "        print(\"  Missing non-imputed files in NIR:\", len(nir_missing))\n",
    "        print(\"  Missing imputed files in NIR_imputed:\", len(nir_imputed_missing))\n",
    "        print(\"  Extra files in NIR:\", len(extra_nir))\n",
    "        print(\"  Extra files in NIR_imputed:\", len(extra_nir_imputed))\n",
    "        \n",
    "        # Save missing imputed files to CSV\n",
    "        if nir_imputed_missing:\n",
    "            output_file = os.path.join(base_dir, f\"{folder}_missing_imputed.csv\")\n",
    "            save_missing_imputed(nir_imputed_missing, output_file)\n",
    "            print(f\"  Saved list of missing imputed files to: {output_file}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Base directory\n",
    "output_base_dir = r'C:\\TjallingData\\greenearthnet_additional'\n",
    "\n",
    "# Folders to check\n",
    "folders_to_check = [\n",
    "    'iid_chopped',\n",
    "    'ood-s_chopped',\n",
    "    'ood-st_chopped',\n",
    "    'ood-t_chopped',\n",
    "    'val_chopped'\n",
    "]\n",
    "\n",
    "check_file_consistency(output_base_dir, folders_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I append those from the created .csv lists and impute them using more refined code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "\n",
    "def load_csv_data(csv_file):\n",
    "    data = {}\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            minicube_name = '_'.join(row['file_name'].split('_')[1:5])\n",
    "            if minicube_name not in data:\n",
    "                data[minicube_name] = []\n",
    "            data[minicube_name].append(row)\n",
    "    return data\n",
    "\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%d/%m/%Y').date()\n",
    "    except ValueError:\n",
    "        return datetime.strptime(date_str, '%Y-%m-%d').date()\n",
    "\n",
    "def process_missing_imputed(base_dir, folder, missing_imputed_csv):\n",
    "    input_folder = os.path.join(base_dir, folder, 'NIR')\n",
    "    output_folder = os.path.join(base_dir, folder, 'NIR_imputed')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    csv_file = os.path.join(base_dir, folder, f\"{folder}_detailed.csv\")\n",
    "    csv_data = load_csv_data(csv_file)\n",
    "    \n",
    "    with open(missing_imputed_csv, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            missing_file = row[0]\n",
    "            \n",
    "            # Extract minicube_name from missing_file\n",
    "            minicube_name = '_'.join(missing_file.split('_')[3:7])\n",
    "            \n",
    "            if minicube_name not in csv_data:\n",
    "                print(f\"Warning: Minicube {minicube_name} not found in CSV data\")\n",
    "                continue\n",
    "            \n",
    "            minicube_data = csv_data[minicube_name]\n",
    "            minicube_data.sort(key=lambda x: parse_date(x['date']))\n",
    "            \n",
    "            existing_dates = [parse_date(row['date']) for row in minicube_data if row['imputed'] == 'no']\n",
    "            \n",
    "            # Skip if there are fewer than 2 non-imputed images\n",
    "            if len(existing_dates) < 2:\n",
    "                print(f\"Warning: Not enough images to impute for {minicube_name}\")\n",
    "                continue\n",
    "            \n",
    "            all_dates = [existing_dates[0] + timedelta(days=5*i) for i in range(30)]\n",
    "            missing_date = datetime.strptime(missing_file.split('_')[-1].split('.')[0], '%Y-%m-%d').date()\n",
    "            \n",
    "            first_date = min(existing_dates)\n",
    "            last_date = max(existing_dates)\n",
    "            \n",
    "            before = max((d for d in existing_dates if d <= missing_date), default=first_date)\n",
    "            after = min((d for d in existing_dates if d >= missing_date), default=last_date)\n",
    "            \n",
    "            before_row = next(row for row in minicube_data if parse_date(row['date']) == before)\n",
    "            after_row = next(row for row in minicube_data if parse_date(row['date']) == after)\n",
    "            \n",
    "            before_img = open_image_with_fallback(input_folder, before_row['file_name'])\n",
    "            after_img = open_image_with_fallback(input_folder, after_row['file_name'])\n",
    "            \n",
    "            if before_img is None or after_img is None:\n",
    "                print(f\"Warning: Could not impute image {missing_file} due to missing reference images\")\n",
    "                continue\n",
    "            \n",
    "            total_gap = (after - before).days\n",
    "            current_gap = (missing_date - before).days\n",
    "            \n",
    "            imputed_arr = impute_image(before_img, after_img, total_gap, current_gap)\n",
    "            imputed_img = Image.fromarray(imputed_arr)\n",
    "            \n",
    "            output_path = os.path.join(output_folder, missing_file)\n",
    "            imputed_img.save(output_path)\n",
    "            print(f\"Saved imputed image: {output_path}\")\n",
    "\n",
    "# Base directory\n",
    "output_base_dir = r'C:\\TjallingData\\greenearthnet_additional'\n",
    "\n",
    "# Folders to process\n",
    "folders_to_process = [\n",
    "    'iid_chopped',\n",
    "    'ood-s_chopped',\n",
    "    'ood-st_chopped',\n",
    "    'ood-t_chopped',\n",
    "    'val_chopped',\n",
    "    'train'\n",
    "]\n",
    "\n",
    "for folder in folders_to_process:\n",
    "    missing_imputed_csv = os.path.join(output_base_dir, f\"{folder}_missing_imputed.csv\")\n",
    "    if os.path.exists(missing_imputed_csv):\n",
    "        print(f\"Processing missing imputed images for {folder}\")\n",
    "        process_missing_imputed(output_base_dir, folder, missing_imputed_csv)\n",
    "    else:\n",
    "        print(f\"No missing imputed CSV found for {folder}\")\n",
    "\n",
    "print(\"Imputation process completed for all folders.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we combine the NIR, NIR_imputed, RGB, and RGB_imputed into one single directory respectively, ready for preprocessing and training. We check in the metadata.csv file of each separate track to check if the file exists in it. If so, it is moved. This functions as an extra layer of control, making sure only images present in the .csv file are moved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Base path\n",
    "base_path = r\"C:\\TjallingData\\greenearthnet_additional\"\n",
    "\n",
    "subfolders =  ['val_chopped', 'iid_chopped', 'ood-s_chopped', 'ood-st_chopped', 'ood-t_chopped', 'train']\n",
    "\n",
    "\n",
    "def copy_folders(subfolder_path, source_folders, target_folder):\n",
    "    target_path = os.path.join(subfolder_path, target_folder)\n",
    "\n",
    "    # Create target folder if it doesn't exist\n",
    "    if not os.path.exists(target_path):\n",
    "        os.makedirs(target_path)\n",
    "\n",
    "    # Copy contents of source folders\n",
    "    for source_folder in source_folders:\n",
    "        source_path = os.path.join(subfolder_path, source_folder)\n",
    "        if os.path.exists(source_path):\n",
    "            for item in os.listdir(source_path):\n",
    "                s = os.path.join(source_path, item)\n",
    "                d = os.path.join(target_path, item)\n",
    "                if os.path.isdir(s):\n",
    "                    shutil.copytree(s, d, dirs_exist_ok=True)\n",
    "                else:\n",
    "                    shutil.copy2(s, d)\n",
    "\n",
    "    print(f\"Copied contents of {', '.join(source_folders)} to {target_folder} in {subfolder_path}\")\n",
    "\n",
    "# Process each subfolder\n",
    "for subfolder in tqdm(subfolders, desc=\"Processing subfolders\"):\n",
    "    subfolder_path = os.path.join(base_path, subfolder)\n",
    "    input_file = os.path.join(subfolder_path, f\"{subfolder}_combined_imputed_with_region_normalized_and_season.csv\")\n",
    "    output_file = os.path.join(subfolder_path, f\"{subfolder}_combined_imputed_with_region_normalized_season_minicube_and_usable.csv\")\n",
    "\n",
    "    if os.path.exists(input_file):\n",
    "        print(f\"\\nProcessing {subfolder}\")\n",
    "\n",
    "        # Copy NIR folders\n",
    "        copy_folders(subfolder_path, ['NIR', 'NIR_imputed'], 'NIR_total')\n",
    "\n",
    "        # Copy RGB folders\n",
    "        copy_folders(subfolder_path, ['RGB', 'RGB_imputed'], 'RGB_total')\n",
    "\n",
    "print(\"\\nProcessing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
