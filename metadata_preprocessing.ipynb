{"cells":[{"cell_type":"markdown","metadata":{"id":"s1XYcwabmr3Y"},"source":["This notebook takes care of preparing all necessary metadata for training and testing. Specifically, a metadata file is created containing all information used during both.\n","\n","\n","All necessary variables are:\n","\n","1. usable\n","2. file_name\n","3. frame_date\n","4. number\n","5. temperature_avg\n","6. sea-level pressure\n","7. shortwave downwelling radiation\n","8. relative humidity\n","9. region\n","10. season\n","\n","\n","\n","First, we impute and normalize all numerical climate data in each metadata.csv file to make sure there are no NaNs. Imputation method: the class average."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p79XRa2emr3d","outputId":"e8371ba1-25d6-4342-ced7-2aab8aacce10"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","\n","# Define the base path and subfolder names\n","base_path = r\"C:\\TjallingData\\greenearthnet_additional\"\n","subfolders = ['val_chopped', 'iid_chopped', 'ood-s_chopped', 'ood-st_chopped', 'ood-t_chopped', 'train']\n","\n","# Columns to process (all numeric columns except 'file_name')\n","columns_to_process = ['Wind Speed', 'Relative Humidity', 'Rainfall', 'Sea-level Pressure',\n","                      'Shortwave Downwelling Radiation', 'Temperature Avg', 'Temperature Min', 'Temperature Max']\n","\n","def impute_and_normalize(df, columns):\n","    # Step 1: Impute missing values with column mean\n","    for column in columns:\n","        df[column] = df[column].fillna(df[column].mean())\n","\n","    # Step 2: Normalize the data\n","    min_values = df[columns].min()\n","    max_values = df[columns].max()\n","\n","    for column in columns:\n","        df[column] = (df[column] - min_values[column]) / (max_values[column] - min_values[column])\n","\n","    return df\n","\n","for subfolder in subfolders:\n","    folder_path = os.path.join(base_path, subfolder)\n","\n","    # Read the metadata CSV file\n","    metadata_file = os.path.join(folder_path, 'metadata.csv')\n","    metadata_df = pd.read_csv(metadata_file)\n","\n","    # Impute and normalize the data\n","    metadata_df = impute_and_normalize(metadata_df, columns_to_process)\n","\n","    # Save the normalized dataframe to a new CSV file\n","    output_file = os.path.join(folder_path, 'metadata_normalized.csv')\n","    metadata_df.to_csv(output_file, index=False)\n","\n","    print(f\"Created {output_file}\")\n","\n","print(\"All metadata files processed and normalized successfully.\")"]},{"cell_type":"markdown","metadata":{"id":"nAmhsVczmr3g"},"source":["\n","Then we merge _detailed.csv and metadata.csv together into a new .csv file that contains file_name, frame_date, number, temperature_avg, sea-level pressure, shortwave downwelling radiation, and relative humidity."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nna6aOCmmr3h","outputId":"e3aa84d5-00c3-4b75-baee-04e4d300869f"},"outputs":[],"source":["import os\n","import pandas as pd\n","\n","# Define the base path and subfolder names\n","base_path = r\"C:\\TjallingData\\greenearthnet_additional\"\n","subfolders =  ['val_chopped', 'iid_chopped', 'ood-s_chopped', 'ood-st_chopped', 'ood-t_chopped', 'train']\n","\n","\n","# Columns to copy from metadata.csv\n","metadata_columns = ['Temperature Avg', 'Sea-level Pressure', 'Shortwave Downwelling Radiation', 'Relative Humidity']\n","\n","for subfolder in subfolders:\n","    folder_path = os.path.join(base_path, subfolder)\n","\n","    # Read the detailed CSV file\n","    detailed_csv = f\"{subfolder}_detailed.csv\"\n","    detailed_df = pd.read_csv(os.path.join(folder_path, detailed_csv))\n","\n","    # Read the metadata CSV file\n","    metadata_df = pd.read_csv(os.path.join(folder_path, 'metadata_normalized.csv'))\n","\n","    # Merge the dataframes based on 'file_name'\n","    combined_df = pd.merge(detailed_df, metadata_df[['file_name'] + metadata_columns], on='file_name', how='left')\n","\n","    # Save the combined dataframe to a new CSV file\n","    output_file = os.path.join(folder_path, f\"{subfolder}_combined.csv\")\n","    combined_df.to_csv(output_file, index=False)\n","\n","    print(f\"Created {output_file}\")\n","\n","print(\"All files processed successfully.\")"]},{"cell_type":"markdown","metadata":{"id":"oMy-wHGimr3i"},"source":["Next, the imputed images their metadata will be imputed. This happens in the same way it happened for the images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MB37-eDUmr3i","outputId":"173af3a4-5070-411f-c10c-38c13ce2ac45"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","\n","def impute_climate_data(input_csv, output_csv):\n","    # Read the CSV file\n","    df = pd.read_csv(input_csv)\n","\n","    # Convert date to datetime if it exists, otherwise use file_name to extract date\n","    if 'date' in df.columns:\n","        df['date'] = pd.to_datetime(df['date'])\n","    else:\n","        df['date'] = pd.to_datetime(df['file_name'].str.extract(r'(\\d{4}-\\d{2}-\\d{2})')[0])\n","\n","    # Extract minicube identifier from file_name\n","    df['minicube'] = df['file_name'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n","\n","    # Sort the dataframe by minicube and date\n","    df = df.sort_values(['minicube', 'date'])\n","\n","    # List of climate columns to impute\n","    climate_columns = ['Temperature Avg', 'Sea-level Pressure', 'Shortwave Downwelling Radiation', 'Relative Humidity']\n","\n","    # Group by minicube\n","    for minicube, group in df.groupby('minicube'):\n","        # For each climate column\n","        for column in climate_columns:\n","            # Get non-NaN values and their dates\n","            known_values = group.loc[group[column].notna(), column]\n","            known_dates = group.loc[group[column].notna(), 'date']\n","\n","            if len(known_values) > 0:  # If there are known values in this minicube\n","                # Convert dates to numbers (days since the earliest date)\n","                date_nums = (group['date'] - group['date'].min()).dt.days\n","                known_date_nums = (known_dates - group['date'].min()).dt.days\n","                impute_date_nums = date_nums[group[column].isna()]\n","\n","                # Interpolate values for imputed dates\n","                imputed_values = np.interp(\n","                    impute_date_nums,\n","                    known_date_nums,\n","                    known_values\n","                )\n","\n","                # Assign interpolated values to the dataframe\n","                df.loc[group[group[column].isna()].index, column] = imputed_values\n","\n","    # Remove the temporary 'minicube' column\n","    df = df.drop('minicube', axis=1)\n","\n","    # Save the updated dataframe\n","    df.to_csv(output_csv, index=False)\n","    print(f\"Imputed data saved to {output_csv}\")\n","\n","# Define the base path and subfolder names\n","base_path = r\"C:\\TjallingData\\greenearthnet_additional\"\n","subfolders = ['val_chopped', 'iid_chopped', 'ood-s_chopped', 'ood-st_chopped', 'ood-t_chopped', 'train']\n","\n","for subfolder in subfolders:\n","    folder_path = os.path.join(base_path, subfolder)\n","    input_csv = os.path.join(folder_path, f\"{subfolder}_combined.csv\")\n","    output_csv = os.path.join(folder_path, f\"{subfolder}_combined_imputed.csv\")\n","\n","    impute_climate_data(input_csv, output_csv)\n","\n","print(\"All files processed and imputed successfully.\")"]},{"cell_type":"markdown","metadata":{"id":"SBxw-Lldmr3j"},"source":["Next, we scrape the location of each minicube and convert those to a location that we can use in the prompt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfMwcAfHmr3k","outputId":"9359701f-7cbe-4b4d-84b9-fe900c47e87a"},"outputs":[],"source":["import os\n","import pandas as pd\n","import xarray as xr\n","from geopy.geocoders import Nominatim\n","from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n","import time\n","from tqdm import tqdm\n","import gc\n","\n","def get_region(lat, lon, geolocator, cache):\n","    cache_key = f\"{lat},{lon}\"\n","    if cache_key in cache:\n","        return cache[cache_key]\n","\n","    tries = 0\n","    while tries < 3:\n","        try:\n","            location = geolocator.reverse(f\"{lat}, {lon}\", language=\"en\")\n","            if location:\n","                address = location.raw['address']\n","                region = address.get('state', address.get('county', \"Unknown\"))\n","                cache[cache_key] = region\n","                return region\n","            return \"Unknown\"\n","        except (GeocoderTimedOut, GeocoderServiceError):\n","            tries += 1\n","            time.sleep(1)\n","    return \"Unknown\"\n","\n","def process_test_track(combined_imputed_path, minicube_folder, subfolder):\n","    geolocator = Nominatim(user_agent=\"minicube_processor\")\n","    cache = {}\n","\n","    # Read the combined_imputed CSV file\n","    df = pd.read_csv(combined_imputed_path)\n","\n","    # Ensure 'region' column exists\n","    if 'region' not in df.columns:\n","        df['region'] = 'Unknown'\n","\n","    # Process each .nc file in the minicube folder and its subfolders\n","    for root, dirs, files in os.walk(minicube_folder):\n","        for file in tqdm(files, desc=f\"Processing {subfolder}\"):\n","            if file.endswith('.nc'):\n","                minicube = file[:-3]  # Remove .nc extension\n","                nc_file = os.path.join(root, file)\n","\n","                try:\n","                    with xr.open_dataset(nc_file) as ds:\n","                        lat = float(ds.lat.mean().values)\n","                        lon = float(ds.lon.mean().values)\n","                        region = get_region(lat, lon, geolocator, cache)\n","                        print(f\"Processed minicube: {minicube}, Region: {region}\")\n","\n","                        # Update the DataFrame directly for each corresponding file\n","                        mask = df['file_name'].apply(lambda x: '_'.join(os.path.basename(x).split('_')[1:-1]) == minicube)\n","                        df.loc[mask, 'region'] = region\n","\n","                        # Save progress after each processed minicube\n","                        output_file = os.path.join(os.path.dirname(combined_imputed_path), f\"{subfolder}_combined_imputed_with_region.csv\")\n","                        df.to_csv(output_file, index=False)\n","                        print(f\"Updated DataFrame and saved to {output_file}\")\n","\n","                except Exception as e:\n","                    print(f\"Error processing minicube file {nc_file}: {e}\")\n","\n","                # Clear some memory\n","                gc.collect()\n","\n","# Base paths\n","base_path = r\"C:\\TjallingData\\greenearthnet_additional\"\n","minicube_base_path = r\"C:\\Workdir\\Develop\\greenearthnet\"\n","\n","subfolders = ['val_chopped', 'iid_chopped', 'ood-s_chopped', 'ood-st_chopped', 'ood-t_chopped', 'train']\n","\n","\n","# Process each test track\n","for subfolder in subfolders:\n","    combined_imputed_path = os.path.join(base_path, subfolder, f\"{subfolder}_combined_imputed.csv\")\n","    minicube_folder = os.path.join(minicube_base_path, subfolder)\n","\n","    print(f\"\\nProcessing {subfolder}...\")\n","    process_test_track(combined_imputed_path, minicube_folder, subfolder)\n","\n","print(\"\\nAll test tracks processed successfully.\")"]},{"cell_type":"markdown","metadata":{"id":"1Ko5CbQ7mr3l"},"source":["\n","We now filter all region names and convert them to quality text, since certain symbols are expressed weirdly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0hcsMddomr3m","outputId":"7524f5f2-90e6-436d-ccf7-b748d164d3a9"},"outputs":[],"source":["import csv\n","import re\n","import unicodedata\n","import os\n","from tqdm import tqdm\n","\n","def normalize_region_name(name):\n","    # Decode the string to handle the specific problematic sequences correctly\n","    name = name.encode('latin-1').decode('utf-8', errors='ignore')\n","\n","    # Specific problematic cases for manual replacement\n","    replacements = {\n","        'Ã©': 'é', 'Ã¼': 'ü', 'Ã³': 'ó', 'Ã': 'í',\n","        'â\\x80\\x93': '–', 'Ã¶': 'ö', 'Ã¡': 'á',\n","        'Ã¤': 'ä', 'Ã¢': 'â', 'Ã´': 'ô', 'Ã±': 'ñ'\n","    }\n","    for old, new in replacements.items():\n","        name = name.replace(old, new)\n","\n","    # Normalize the Unicode characters\n","    normalized_name = unicodedata.normalize('NFKD', name)\n","\n","    # Remove non-ASCII characters but keep spaces and common punctuation\n","    ascii_name = ''.join(char for char in normalized_name if ord(char) < 128 or char in (' ', '-', '/'))\n","\n","    # Replace multiple spaces with a single space\n","    ascii_name = re.sub(r'\\s+', ' ', ascii_name)\n","\n","    # Remove leading/trailing spaces\n","    ascii_name = ascii_name.strip()\n","\n","    return ascii_name\n","\n","def process_csv_file(input_file, output_file):\n","    with open(input_file, 'r', newline='', encoding='latin-1') as infile, \\\n","         open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n","\n","        reader = csv.reader(infile)\n","        writer = csv.writer(outfile)\n","\n","        # Read the header\n","        header = next(reader)\n","        writer.writerow(header)\n","\n","        # Find the index of the 'region' column\n","        region_index = header.index('region')\n","\n","        for row in reader:\n","            # Normalize the region name\n","            row[region_index] = normalize_region_name(row[region_index])\n","            writer.writerow(row)\n","\n","# Base path\n","base_path = r\"C:\\TjallingData\\greenearthnet_additional\"\n","\n","subfolders = ['val_chopped', 'iid_chopped', 'ood-s_chopped', 'ood-st_chopped', 'ood-t_chopped', 'train']\n","\n","# Process each subfolder\n","for subfolder in tqdm(subfolders, desc=\"Processing subfolders\"):\n","    input_file = os.path.join(base_path, subfolder, f\"{subfolder}_combined_imputed_with_region.csv\")\n","    output_file = os.path.join(base_path, subfolder, f\"{subfolder}_combined_imputed_with_region_normalized.csv\")\n","\n","    if os.path.exists(input_file):\n","        print(f\"\\nProcessing: {input_file}\")\n","        process_csv_file(input_file, output_file)\n","        print(f\"Normalized data has been written to {output_file}\")\n","    else:\n","        print(f\"\\nFile not found: {input_file}\")\n","\n","print(\"\\nAll files processed successfully.\")"]},{"cell_type":"markdown","metadata":{"id":"QUYCYe0imr3n"},"source":["Next, we create the season variable by deducing it from each date the image was taken."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2y5j3Acmr3n","outputId":"9cd231b9-9cff-46f4-ed35-19a576718acd"},"outputs":[],"source":["import os\n","import pandas as pd\n","from datetime import datetime\n","from tqdm import tqdm\n","\n","def get_season(date):\n","    # Convert string to datetime if necessary\n","    if isinstance(date, str):\n","        date = datetime.strptime(date, '%Y-%m-%d')\n","\n","    # Get day of year\n","    doy = date.timetuple().tm_yday\n","\n","    # Define day ranges for seasons\n","    spring = range(80, 172)\n","    summer = range(172, 264)\n","    fall = range(264, 355)\n","    # winter = everything else\n","\n","    # Define ranges for early, mid, late within each season\n","    def get_sub_season(start, end):\n","        third = (end - start + 1) // 3\n","        early = range(start, start + third)\n","        mid = range(start + third, start + 2*third)\n","        late = range(start + 2*third, end + 1)\n","        return early, mid, late\n","\n","    spring_early, spring_mid, spring_late = get_sub_season(80, 171)\n","    summer_early, summer_mid, summer_late = get_sub_season(172, 263)\n","    fall_early, fall_mid, fall_late = get_sub_season(264, 354)\n","    winter_early, winter_mid, winter_late = get_sub_season(355, 79)\n","\n","    if doy in spring:\n","        if doy in spring_early:\n","            return 'early spring'\n","        elif doy in spring_mid:\n","            return 'mid spring'\n","        else:\n","            return 'late spring'\n","    elif doy in summer:\n","        if doy in summer_early:\n","            return 'early summer'\n","        elif doy in summer_mid:\n","            return 'mid summer'\n","        else:\n","            return 'late summer'\n","    elif doy in fall:\n","        if doy in fall_early:\n","            return 'early fall'\n","        elif doy in fall_mid:\n","            return 'mid fall'\n","        else:\n","            return 'late fall'\n","    else:  # winter\n","        if doy in winter_early or doy in range(1, winter_early.stop):\n","            return 'early winter'\n","        elif doy in winter_mid:\n","            return 'mid winter'\n","        else:\n","            return 'late winter'\n","\n","def add_season_to_csv(input_file, output_file):\n","    # Read the CSV file\n","    df = pd.read_csv(input_file)\n","\n","    # Convert 'date' to datetime\n","    df['date'] = pd.to_datetime(df['date'])\n","\n","    # Apply the function to create the new 'season' column\n","    df['season'] = df['date'].apply(get_season)\n","\n","    # Save the updated DataFrame back to CSV\n","    df.to_csv(output_file, index=False)\n","    print(f\"Processed: {output_file}\")\n","\n","# Base path\n","base_path = r\"C:\\TjallingData\\greenearthnet_additional\"\n","\n","subfolders =  ['val_chopped', 'iid_chopped', 'ood-s_chopped', 'ood-st_chopped', 'ood-t_chopped', 'train']\n","\n","# Process each subfolder\n","for subfolder in tqdm(subfolders, desc=\"Processing subfolders\"):\n","    input_file = os.path.join(base_path, subfolder, f\"{subfolder}_combined_imputed_with_region_normalized.csv\")\n","    output_file = os.path.join(base_path, subfolder, f\"{subfolder}_combined_imputed_with_region_normalized_and_season.csv\")\n","\n","    if os.path.exists(input_file):\n","        add_season_to_csv(input_file, output_file)\n","    else:\n","        print(f\"File not found: {input_file}\")\n","\n","print(\"\\nAll files processed successfully.\")"]},{"cell_type":"markdown","metadata":{"id":"NcYpPRrvmr3o"},"source":["Finally, we make a selection of minicubes we can use during testing: we can modify the selection criteria here, to omit any bad test sets containing less than 30% non-imputed images. It also adds an extra variable: 'minicube'.\n","\n","\n","Currently selecting on minimum 3 context and 6 target images, which means a total of 30% of all images need to be non-imputed. The same threshold is used in the source code of the EarthNetScore package.\n","\n","Important note: The minimum threshold differs between the training set and test sets. For the training set, a minimum image count was set to 5 context and 10 target images (50%). This was done to ensure a larger training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYIraXZDmr3p","outputId":"06061137-89b6-4db2-9d3e-c89e4f0ca6de"},"outputs":[],"source":["import os\n","import csv\n","from collections import defaultdict\n","from tqdm import tqdm\n","\n","context_img_required = 3\n","target_img_required = 6\n","\n","def process_csv_file(input_file, output_file):\n","    # Dictionary to store minicube information\n","    minicubes = defaultdict(lambda: {'context_no_impute': 0, 'target_no_impute': 0})\n","\n","    # Read the input CSV file and process each row\n","    with open(input_file, 'r') as csvfile:\n","        reader = csv.DictReader(csvfile)\n","        for row in reader:\n","            file_name = row['file_name']\n","            property_type = row['property']\n","            imputed = row['imputed']\n","\n","            # Extract minicube name by removing the .png suffix and the date\n","            minicube_name = '_'.join(file_name.split('_')[:-1])\n","            minicube_name = minicube_name.rsplit('.', 1)[0]  # Remove .png\n","\n","            # Increment the appropriate counter only if not imputed\n","            if imputed.lower() == 'no':\n","                if property_type == 'context':\n","                    minicubes[minicube_name]['context_no_impute'] += 1\n","                elif property_type == 'target':\n","                    minicubes[minicube_name]['target_no_impute'] += 1\n","\n","    print(f\"Debug: Number of unique minicubes: {len(minicubes)}\")\n","    print(f\"Debug: First 5 minicubes and their counts: {list(minicubes.items())[:5]}\")\n","\n","    # Write the results to the output CSV file\n","    with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n","        reader = csv.DictReader(infile)\n","        fieldnames = reader.fieldnames + ['minicube', 'usable']\n","        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n","\n","        writer.writeheader()\n","        for row in reader:\n","            file_name = row['file_name']\n","            minicube_name = '_'.join(file_name.split('_')[:-1])\n","            minicube_name = minicube_name.rsplit('.', 1)[0]  # Remove .png\n","\n","            # Determine if the minicube is usable\n","            counts = minicubes[minicube_name]\n","            usable = 'yes' if counts['context_no_impute'] >= context_img_required and counts['target_no_impute'] >= target_img_required else 'no'\n","\n","            # Add the 'minicube' and 'usable' columns to the row\n","            row['minicube'] = minicube_name\n","            row['usable'] = usable\n","            writer.writerow(row)\n","\n","    return minicubes\n","\n","# Base path\n","base_path = r\"C:\\TjallingData\\greenearthnet_additional\"\n","\n","subfolders =  ['val_chopped', 'iid_chopped', 'ood-s_chopped', 'ood-st_chopped', 'ood-t_chopped', 'train']\n","\n","\n","# Process each subfolder\n","for subfolder in tqdm(subfolders, desc=\"Processing subfolders\"):\n","    input_file = os.path.join(base_path, subfolder, f\"{subfolder}_combined_imputed_with_region_normalized_and_season.csv\")\n","    output_file = os.path.join(base_path, subfolder, f\"{subfolder}_combined_imputed_with_region_normalized_season_minicube_and_usable.csv\")\n","\n","    if os.path.exists(input_file):\n","        print(f\"\\nProcessing {subfolder}\")\n","        minicubes = process_csv_file(input_file, output_file)\n","\n","        # Count usable and non-usable minicubes\n","        usable_count = sum(1 for counts in minicubes.values() if counts['context_no_impute'] >= context_img_required and counts['target_no_impute'] >= target_img_required)\n","        total_count = len(minicubes)\n","\n","        print(f\"\\nResults for {subfolder}:\")\n","        print(f\"Total minicubes: {total_count}\")\n","        print(f\"Usable minicubes: {usable_count}\")\n","        print(f\"Non-usable minicubes: {total_count - usable_count}\")\n","        print(f\"Usable percentage: {usable_count / total_count * 100:.2f}%\")\n","\n","        # Debug: Print some sample minicubes and their counts\n","        print(\"\\nDebug: Sample minicubes and their counts:\")\n","        for minicube, counts in list(minicubes.items())[:5]:\n","            print(f\"{minicube}: context_no_impute={counts['context_no_impute']}, target_no_impute={counts['target_no_impute']}\")\n","    else:\n","        print(f\"File not found: {input_file}\")\n","\n","print(\"\\nProcessing complete.\")"]},{"cell_type":"markdown","metadata":{"id":"mth8y90cmr3p"},"source":["Moving both NIR imputed and NIR non-imputed to a common folder per track."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G3QOY6LZmr3q","outputId":"a2dc7e31-aafc-4c44-dc29-6ab00e26a81f"},"outputs":[],"source":["import os\n","import csv\n","from collections import defaultdict\n","from tqdm import tqdm\n","import shutil\n","\n","\n","# Base path\n","base_path = r\"C:\\TjallingData\\greenearthnet_additional\"\n","\n","subfolders = ['val_chopped', 'iid_chopped', 'ood-s_chopped', 'ood-st_chopped', 'ood-t_chopped', 'train']\n","\n","def copy_folders(subfolder_path, source_folders, target_folder):\n","    target_path = os.path.join(subfolder_path, target_folder)\n","\n","    # Create target folder if it doesn't exist\n","    if not os.path.exists(target_path):\n","        os.makedirs(target_path)\n","\n","    # Copy contents of source folders\n","    for source_folder in source_folders:\n","        source_path = os.path.join(subfolder_path, source_folder)\n","        if os.path.exists(source_path):\n","            for item in os.listdir(source_path):\n","                s = os.path.join(source_path, item)\n","                d = os.path.join(target_path, item)\n","                if os.path.isdir(s):\n","                    shutil.copytree(s, d, dirs_exist_ok=True)\n","                else:\n","                    shutil.copy2(s, d)\n","\n","    print(f\"Copied contents of {', '.join(source_folders)} to {target_folder} in {subfolder_path}\")\n","\n","# Process each subfolder\n","for subfolder in tqdm(subfolders, desc=\"Processing subfolders\"):\n","    subfolder_path = os.path.join(base_path, subfolder)\n","    input_file = os.path.join(subfolder_path, f\"{subfolder}_combined_imputed_with_region_normalized_and_season.csv\")\n","    output_file = os.path.join(subfolder_path, f\"{subfolder}_combined_imputed_with_region_normalized_season_minicube_and_usable.csv\")\n","    \n","    if os.path.exists(input_file):\n","        print(f\"\\nProcessing {subfolder}\")\n","        \n","        # Copy NIR folders\n","        copy_folders(subfolder_path, ['NIR', 'NIR_imputed'], 'NIR_total')\n","        \n","        # Copy RGB folders\n","        copy_folders(subfolder_path, ['RGB', 'RGB_imputed'], 'RGB_total')\n","\n","print(\"\\nProcessing complete.\")"]},{"cell_type":"markdown","metadata":{"id":"R1-YCII_mr3z"},"source":["Next, we remove 'RGB_imputed' and 'NIR_imputed' from all imputed images in the _total folders:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tYs3jqH2mr3z","outputId":"9da1c27c-b1f0-4b27-b20a-d1700f1025d4"},"outputs":[],"source":["import os\n","\n","def remove_imputed_prefix(base_dir):\n","    folders = ['NIR_total', 'RGB_total']\n","    prefixes = ['NIR_imputed_', 'RGB_imputed_']\n","\n","    for folder in folders:\n","        folder_path = os.path.join(base_dir, folder)\n","        if not os.path.exists(folder_path):\n","            print(f\"Folder not found: {folder_path}\")\n","            continue\n","\n","        print(f\"Processing folder: {folder_path}\")\n","        for filename in os.listdir(folder_path):\n","            for prefix in prefixes:\n","                if filename.startswith(prefix):\n","                    old_path = os.path.join(folder_path, filename)\n","                    new_filename = filename.replace(prefix, '', 1)\n","                    new_path = os.path.join(folder_path, new_filename)\n","\n","                    try:\n","                        os.rename(old_path, new_path)\n","                        print(f\"Renamed: {filename} -> {new_filename}\")\n","                    except Exception as e:\n","                        print(f\"Error renaming {filename}: {str(e)}\")\n","\n","                    break  # Stop checking prefixes once one is found and removed\n","\n","    print(\"Finished removing prefixes.\")\n","\n","# Usage\n","base_directory = r'C:\\TjallingData\\greenearthnet_additional\\iid_chopped'\n","remove_imputed_prefix(base_directory)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
