{"cells":[{"cell_type":"markdown","metadata":{},"source":["This is where the RGB channels are extracted, overlayed into an RGB.png image, alongside with the corresponding climate data, DEM maps, and landcover maps."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0IAFjLfZztJb"},"outputs":[],"source":["import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import xarray as xr\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","from pathlib import Path\n","import gc\n","import dask.array as da\n","\n","# List of directories to process\n","input_dirs = [\n","    r'C:\\Workdir\\Develop\\greenearthnet\\val_chopped',\n","    r'C:\\Workdir\\Develop\\greenearthnet\\ood-st_chopped',\n","    r'C:\\Workdir\\Develop\\greenearthnet\\ood-t_chopped',\n","    r'C:\\Workdir\\Develop\\greenearthnet\\ood-s_chopped',\n","    r'C:\\Workdir\\Develop\\greenearthnet\\val_chopped',\n","    r'C:\\Workdir\\Develop\\greenearthnet\\train',\n","\n","]\n","output_base_dir = r'C:\\TjallingData\\greenearthnet_additional'\n","\n","# all landcover classes and corresponding colors\n","land_cover_classes = {\n","    10: 'Tree cover', 20: 'Shrubland', 30: 'Grassland', 40: 'Cropland',\n","    50: 'Built-up', 60: 'Bare / sparse vegetation', 70: 'Snow and Ice',\n","    80: 'Permanent water bodies', 90: 'Herbaceous wetland', 95: 'Mangroves',\n","    100: 'Moss and lichen',\n","}\n","\n","colors = ['#006400', '#8db360', '#bae4b3', '#ffffcc', '#ffeda0', '#f2f0f7', '#f7fcf0', '#081d58', '#c7e9b4', '#7fcdbb', '#00441b']\n","cmap = mcolors.ListedColormap(colors)\n","bounds = list(land_cover_classes.keys()) + [max(land_cover_classes.keys()) + 1]\n","norm = mcolors.BoundaryNorm(bounds, cmap.N)\n","\n","# E-OBS variables to plot for the landcover map\n","variables = ['eobs_fg', 'eobs_hu', 'eobs_rr', 'eobs_pp', 'eobs_qq', 'eobs_tg', 'eobs_tn', 'eobs_tx']\n","variable_names = ['Wind Speed', 'Relative Humidity', 'Rainfall', 'Sea-level Pressure', 'Shortwave Downwelling Radiation', 'Temperature Avg', 'Temperature Min', 'Temperature Max']\n","\n","def process_file(nc_file, output_dir, variables, variable_names, subfolder_prefix):\n","    try:\n","        with xr.open_dataset(nc_file) as minicube:\n","            minicube_with_data = minicube.isel(time=slice(4, None, 5))\n","\n","            # Check if 's2_mask' exists\n","            if 's2_dlmask' in minicube_with_data:\n","                minicube_clear = minicube_with_data.where(minicube_with_data.s2_dlmask == 0, drop=True)\n","            else:\n","                print(f\"'s2_dlmask' not found in {nc_file}. Skipping mask filtering.\")\n","                minicube_clear = minicube_with_data\n","\n","            # Check for required bands\n","            bands_to_check = ['s2_B02', 's2_B03', 's2_B04', 's2_B8A']\n","            for band in bands_to_check:\n","                if band not in minicube_clear:\n","                    print(f\"Band {band} not found in {nc_file}. Skipping file.\")\n","                    return None\n","                minicube_clear = minicube_clear.dropna(dim='time', subset=[band], how='any')\n","\n","            num_time_steps = minicube_clear.sizes['time']\n","\n","            if num_time_steps == 0:\n","                print(f\"No valid data found in {nc_file}\")\n","                return None\n","\n","            time_dates = pd.to_datetime(minicube_clear.time.values)\n","\n","            base_name = os.path.splitext(os.path.basename(nc_file))[0]\n","\n","            df_list = []\n","\n","            for i in range(num_time_steps):\n","                date_str = time_dates[i].strftime('%Y-%m-%d')\n","                filename_prefix = f\"{subfolder_prefix}_{base_name}_{date_str}\"\n","\n","                # RGB Image: a composite of the three R G B bands\n","                red = minicube_clear['s2_B04'].isel(time=i).values\n","                green = minicube_clear['s2_B03'].isel(time=i).values\n","                blue = minicube_clear['s2_B02'].isel(time=i).values\n","\n","                # Normalize RGB values\n","                red_normalized = (red - red.min()) / (red.max() - red.min())\n","                green_normalized = (green - green.min()) / (green.max() - green.min())\n","                blue_normalized = (blue - blue.min()) / (blue.max() - blue.min())\n","\n","                rgb_normalized = np.dstack((red_normalized, green_normalized, blue_normalized))\n","\n","                fig, ax = plt.subplots(figsize=(5, 4))\n","                ax.imshow(rgb_normalized)\n","                ax.axis('off')\n","                fig.savefig(os.path.join(output_dir, 'RGB', f'{filename_prefix}.png'), bbox_inches='tight', pad_inches=0, dpi=300)\n","                plt.close(fig)\n","\n","\n","                # Landcover Map\n","                fig, ax = plt.subplots(figsize=(5, 4))\n","                data = minicube_clear['esawc_lc'].isel(time=i).values\n","                ax.imshow(data, cmap=cmap, norm=norm)\n","                ax.axis('off')\n","                fig.savefig(os.path.join(output_dir, 'LandcoverMaps', f'{filename_prefix}.png'), bbox_inches='tight', pad_inches=0, dpi=300)\n","                plt.close(fig)\n","\n","                # DEM Maps\n","                elevation_datasets = {\n","                    'nasa_dem': minicube_clear['nasa_dem'].isel(time=i).values,\n","                    'cop_dem': minicube_clear['cop_dem'].isel(time=i).values,\n","                    'alos_dem': minicube_clear['alos_dem'].isel(time=i).values\n","                }\n","\n","                for elev_name, elev_data in elevation_datasets.items():\n","                    fig, ax = plt.subplots(figsize=(5, 4))\n","                    ax.imshow(elev_data, cmap='terrain')\n","                    ax.axis('off')\n","                    fig.savefig(os.path.join(output_dir, 'DEM', f'{filename_prefix}_{elev_name}.png'), bbox_inches='tight', pad_inches=0, dpi=300)\n","                    plt.close(fig)\n","\n","                # Metadata\n","                df_temp = pd.DataFrame(index=[time_dates[i]])\n","                for var, name in zip(variables, variable_names):\n","                    data_avg = float(minicube_clear[var].isel(time=i).mean().values)\n","                    df_temp[name] = data_avg\n","\n","                df_temp.reset_index(inplace=True)\n","                df_temp.rename(columns={'index': 'file_name'}, inplace=True)\n","                df_temp['file_name'] = f'{filename_prefix}.png'\n","                df_list.append(df_temp)\n","\n","                print(f'Processed {filename_prefix}')\n","\n","            if df_list:\n","                return pd.concat(df_list, ignore_index=True)\n","            else:\n","                print(f\"No valid data frames created for {nc_file}\")\n","                return None\n","    except Exception as e:\n","        print(f\"An error occurred while processing {nc_file}: {e}\")\n","        return None\n","\n","# Main processing loop\n","for input_dir in input_dirs:\n","    map_name = os.path.basename(input_dir)\n","    output_dir = os.path.join(output_base_dir, map_name)\n","\n","    os.makedirs(os.path.join(output_dir, 'RGB'), exist_ok=True)\n","    os.makedirs(os.path.join(output_dir, 'LandcoverMaps'), exist_ok=True)\n","    os.makedirs(os.path.join(output_dir, 'DEM'), exist_ok=True)\n","\n","    # Get all subfolders\n","    subfolders = [f.path for f in os.scandir(input_dir) if f.is_dir()]\n","\n","    for subfolder in subfolders:\n","        subfolder_name = os.path.basename(subfolder)\n","        nc_files = glob.glob(os.path.join(subfolder, '*.nc'))\n","\n","        # Process files in batches\n","        batch_size = 10\n","        for i in range(0, len(nc_files), batch_size):\n","            batch_files = nc_files[i:i+batch_size]\n","            df_list = []\n","\n","            for nc_file in batch_files:\n","                df_temp = process_file(nc_file, output_dir, variables, variable_names, subfolder_name)\n","                if df_temp is not None:\n","                    df_list.append(df_temp)\n","\n","                # Clear memory\n","                gc.collect()\n","\n","            # Concatenate and save batch results\n","            if df_list:\n","                df_batch = pd.concat(df_list, ignore_index=True)\n","                df_batch.to_csv(os.path.join(output_dir, f'metadata_batch_{subfolder_name}_{i//batch_size}.csv'), index=False)\n","\n","            del df_list\n","            gc.collect()\n","\n","        print(f'Completed processing for {map_name}/{subfolder_name}')\n","\n","    # Combine all batch results for the current input_dir\n","    all_batches = glob.glob(os.path.join(output_dir, 'metadata_batch_*.csv'))\n","    if all_batches:\n","        df = pd.concat([pd.read_csv(f) for f in all_batches], ignore_index=True)\n","        df.to_csv(os.path.join(output_dir, 'metadata.csv'), index=False)\n","\n","        # Clean up batch files\n","        for f in all_batches:\n","            os.remove(f)\n","    else:\n","        print(f\"No valid metadata batches found for {map_name}\")\n","\n","print('All processing complete.')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP0U3pI7iumrgY79YgVvuhh","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
